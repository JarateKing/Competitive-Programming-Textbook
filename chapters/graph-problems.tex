\section{Graph Problems}
\subsection{Concepts}
\subsubsection{Nodes and Edges}
\subsubsection{Representations}

There are multiple ways to represent graphs when we talk about actually implementing them in code.

\textbf{Edge lists} are likely the most simple representation of a graph, in which we have an array store each edge in the graph. Nodes are implicit by their value -- an edge is represented as a pair of integers $a$ and $b$ (possibly with additional information like weight) that defines a connection between the $a$th node and the $b$th node.

\inputcpp{code/graph/core_edge_list.cpp}

Unfortunately, edge lists are relative uncommon and aren't the ideal graph representation for the majority of graph algorithms. This is because one of the most common things we want to do with graph algorithms is operate on edges connected to a specific node. With edge lists, this is $O(n)$ -- we have to traverse through the entire list to find what edges are connected to a specific node. We can optimize this somewhat (sort the edge list so all nodes appear in order in $O(n log n)$, then binary search for a specific node in $O(log n)$, for example) but ultimately we can do better with other graph representations for most use cases.

\textbf{Adjacency lists} are a graph representation in which we have an array of each node, where each node stores its edges. This fixes our issue with edge lists, because now we can index directly into a node and operate directly on all of its incident edges. This can either be as an array of \mintinline{cpp}{Node} objects where each \mintinline{cpp}{Node} contains an edge list, or as an array of edge lists. Even simpler, we can have an array where each index is the start node of the edge, and store a list of end node values:

\inputcpp{code/graph/core_adjacency_list.cpp}

Now, we may notice that we improved our accesses for our start node from $O(n)$ to $O(1)$ by using an array instead of a list, with the index equal to the start node of a given edge. Unfortunately, since adjacency lists still use lists, trying determine whether an edge between a specific start node and a specific end node exists is potentially expensive as well -- we would have to iterate through every edge indicent with the start node to find if that specific end node exists. So you may ask, why not use an array again, so that we have a 2D array where end nodes are our index for the inner array?

As it turns out, this is called an \textbf{adjacency matrix}. We maintain a 2D array of boolean values, so that we can quickly check whether an edge between $a$ and $b$ exists by checking the value of \mintinline{cpp}{edges[a][b]}.

\inputcpp{code/graph/core_adjacency_matrix.cpp}

Now, is any one of these representations strictly better than the others? Not at all! They all have their strengths and weaknesses, namely:
\begin{itemize}
\item Only adjacency matrices can check if an edge between $a$ and $b$ exists in $O(1)$. Edge lists need to iterate through the entire list to find if that edge exists, while adjacency lists have to iterate through the list of edges incident to $a$.
\item If there are only a couple of edges incident to node $a$, only adjacency lists can iterate through those incident edges quickly. Edge lists don't have easy access to find node $a$ and have to search through the entire list, while adjacency matrices have to look through every value of \mintinline{cpp}{edges[a][0..n]} to find incident edges.
\item If there are only a couple of edges in general and you need to iterate through every edge, only edge lists can iterate through them quickly. Adjacency lists require you to iterate through its whole array of size $n$, and adjacency matrices require you to iterate through its entire 2D array -- to go through $n^2$ different values.
\end{itemize}
To generalize, using a list is good if your graph doesn't have a large amount of edges compared to the number of nodes, and you want to iterate through edges rather than access specific ones. Comparatively, using an array is good if your graph has lots of edges relative to the number of nodes, and/or you don't need to iterate through edges and would prefer the $O(1)$ lookup time.

In practice, edge lists are usually not the preferred method -- there are relatively few algorithms or problems that are best implemented by iterating through every edge in arbitrary order. Adjacency lists and adjacency matrices appear fairly frequently, so it's important to know both of them well.

\subsubsection{Directed and Undirected}

One trait of graphs is whether they're directed or undirected.

In a directed graph, edges are one way -- an edge from $a$ to $b$ can be traversed in that order, but you could not go from $b$ to $a$ using that edge.

In contrast, an undirected graph has edges that go in either direction. The edge $a,b$ also represents an edge $b,a$ in that traversal can be done with either $a$ to $b$ or $b$ to $a$.

Our graph representations that we developed above only supported directed graphs. However, when we add an edge $a,b$, we can also add the edge $b,a$ in order to make our representations be equivalent to undirected graphs.

\subsubsection{Cyclic and Acyclic}

A \textbf{cycle} is a path where the start and end node are the same. In other words, without traversing the same edges, cycles are what you find when you end up at the same node as you started.

We call a graph that contains cycles \textbf{cyclic}. In comparison, a graph with no cycles is \textbf{acyclic}. This is relevant because sometimes algorithms work well on acyclic graphs but not on cyclic ones, or alternatively fundamentally only matter for cyclic graphs (such as cycle finding algorithms).

\subsubsection{Connected and Disconnected}

Two nodes are considered \textbf{connected} when there exists some path between them. Opposite to this, two nodes are \textbf{disconnected} if there isn't a path between them, and by definition no amount of graph traversing from one node will lead you to the other.

When we're talking about the graph as a whole, a \textbf{connected graph} means that every node is connected to every other node. Likewise, a \textbf{disconnected graph} means that at least one pair of nodes is not connected.

\subsubsection{Complete}

A graph is considered \textbf{complete} if every node contains an edge to every other node. As a result, the minimum path length from one node to another is always $1$ in a complete graph.

A complete graph with $n$ nodes will contain $\frac{n(n-1)}{2}$ edges. While often assumptions can be made about complete graphs that simplify problems immensely, the large amount of edges may also make common algorithms expensive and ineffective.

\subsubsection{Weighted}
\subsubsection{Transitive Closure}
\subsubsection{Grids}
\subsection{Pathfinding}

One of the most common tasks with graphs is to find a path between two nodes, or the distance between them. As we will see, traversing the graph in this manner is a common subtask in several other, more complicated graph algorithms as well.

It's important to know that, in general, there isn't any single pathfinding algorithm that you should always use. Each have their own strengths and weaknesses that are only relevant in certain circumstances, or come with specific restrictions that make them only worthwhile on specific types of graphs.

\subsubsection{Depth First Search}
\subsubsection{Breadth First Search}
\subsubsection{Djikstra's Algorithm}
\subsubsection{A* Algorithm}
\subsubsection{Bellman-Ford Algorithm}
\subsubsection{Floyd-Warshall Algorithm}
\subsection{Union-Find}

Union-Find, which also goes by the name of \textit{disjoint-set}, is a data structure that groups connected nodes together into different subsets through operations called \textbf{union} (which connects two nodes together, akin to creating an edge) and \textbf{find} (which determines which subset a node belongs to).

\subsection{Minimum Spanning Trees}

Many graphs have redundant edges -- between nodes $A$ and $B$ there may be several different paths. A spanning tree of a connected graph is one in which we use a subset of edges (whether we think of it as removing redundant edges or building up only the necessary edges depends on the algorithm we use) so that every pair of nodes has exactly one unique path between them.

A minimum spanning tree is a variant of a general spanning tree, where we want to obtain the minimum total weight among all spanning trees.

\subsubsection{Prim's Algorithm}
\subsubsection{Kruskal's Algorithm}
\subsection{Strongly Connected Components}
\subsubsection{Kosaraju's Algorithm}
\subsubsection{Tarjan's Algorithm}
\subsection{Maximum Flow}
\subsection{Bipartite Graphs}
\subsubsection{Determining Bipartite Graphs}
\subsubsection{Maximum Bipartite Matching}
\subsection{Eulerian Paths}

A Eulerian Path is a traversal of a graph in which every edge is visited exactly once. By consequence each node will also be visited one or more times.

Similarly, a Eulerian Cycle is a traversal of a graph that visits every edge exactly once with the added restriction that it's a cycle and has the same start and end node.

It is relatively easy to verify whether or not a Eulerian Path or Cycle exists. The following conditions must be satisfied:
\begin{itemize}
\item The graph must be connected
\item The degree of each node must be even
\end{itemize}

\subsubsection{BEST Algorithm}
\subsubsection{Hierholzer's Algorithm}
\subsection{Hamiltonian Paths}

A Hamiltonian Path is similar to a Eulerian Path except that it deals with nodes rather than edges -- it's a traversal in which each node is visited exactly once, and by consequence each edge is either visited once or not visited at all.

A Hamiltonian Cycle follows the same convention where we add the restriction that the end node of our traversal also has to be our source node.

Unlike Eulerian Paths and Cycles, verifying whether a Hamiltonian Path or Cycle exists or not is an NP-complete problem for the general case. There are special cases however:
\begin{itemize}
\item a complete graph with $n$ nodes has $(n-1)!/2$ different Hamiltonian Cycles.
\item By Dirac's Theorem, a simple graph with at least 3 nodes contains a Hamiltonian Cycle if every node has a degree of at least $\frac{n}{2}$ where $n$ is the number of nodes in the graph.
\item By Ore's Theorem (which generalizes Dirac's Theorem), a simple graph with at least 3 nodes contains a Hamiltonian Cycle if, for every pair of nodes in the graph, the sum of their degrees is at least $n$ where $n$ is the number of nodes in the graph.
\end{itemize}
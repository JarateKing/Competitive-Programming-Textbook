\section{Advanced Data Structures}

"Advanced" is possibly a misnomer. Some of these data structures aren't very complicated, in either their implementation or their usage. We call them advanced here because they relate to much more specific and particular applications. Things like arrays or sets are necessary to solve many basic problems -- while things like range-based structures may be necessary to efficiently implement an algorithm or solution, the problems they appear in are never as simple.

\subsection{Concepts}

\subsubsection{Persistent Data Structures}

Persistent data structures are ones that allow for not just querying an element, but also being able to query it at a specific time, keeping track of the modifications that have been done to it. For example, if you had a queue with 10 elements and pushed and popped several times, you wouldn't be able to query what the front of the queue is at its initial state with 10 elements. A persistent queue would not only allow you to query it at that state, or after any arbitrary amount of pushes and pops for that matter.

A trivial example of a persistent data structure is simply a list of copies of a complete object for every modification that happens to it. If you add an element to a persistent queue, you keep a copy of the old queue and create a new copy of the list with that element added. Likewise with popping the first element in the queue. You do this 100 times, you now have 100 more copies of the queue at different points in history than you started with. And if you want to query the fron of the queue at some point in history, you can easily refer to the proper state and use that.

Conceptually, this is a persistent data structure -- it managed to keep track of its history, and allows us to query this at any time. It should be noted that this is relatively slow, however -- copying the queue is $O(n)$ and is required each time a modification is made to the queue, which makes it too slow for most problems that may require a persistent data structure. Similarly, it takes a large amount of memory, and in problems with fairly large numbers of elements or operations this has a strong likelihood of using too much memory.

Ideally, persistent data structures involve representing the data in such a way that the history is implicit to the structure itself -- that copying the entire structure is unneeded, and it's preferable to always add to the structure in some way instead.

In our persistent queue's case, a better approach would be to recognize that we only ever can represent a queue as a subarray of all the elements added in order -- if we have an array that contains all the elements that will be added to our queue, we can keep track of the index of the element we consider at the front of our queue, and the total size of our queue from that starting position (alternatively, the end position of our queue, or one more than the index of our last element in the queue). Pushing is a simple matter of incrementing our size by 1, while popping involves incrementing our start position by 1 and decrementing our size by 1 as well. For every push or pop we perform on our persistent queue, we can keep track of the state of the queue as a pair of integers for the start position and the size of the queue:

\subsection{Range-based Structures}
\subsubsection{Prefix Sum Arrays}

Prefix sums are not a particularly complicated data structure. However, they are useful as the most basic form of range-based data structures. As the name suggests, they deal with sums of integers.

If we are given an array of integers, we can generate a prefix sum array by making each element be the cumulative sum of all elements before it in the original array. For example, given an array $arr = \{1,2,3,5,10,3,2,5\}$, our prefix sum is $ps = \{1,3,6,11,21,24,26,31\}$ because each $ps_i$ is equal to $arr_0 + ... + arr_i$.

Generating the prefix sum is a simple process...

Normally, the prefix sum gives us the sum of the array from $[0,i)$. We can obtain the sum of an arbitrary subarray $[l,r)$ by obtaining $[0,r)$ and subtracting $[0,l)$. 

While for some problems prefix sums are sufficient (construction in $O(n)$ and queries in $O(1)$), they aren't able to be updated efficiently (you would have to recalculate all sums after the element you're updating, which is $O(n)$).

\subsubsection{Sparse Table}
\subsubsection{Fenwick Tree / Binary Indexed Tree}

Fenwick Trees, also commonly known as Binary Indexed Trees (which is also commonly abbreviated to BITs) is a structure that allows calculates the prefix sum that can be queried in $O(log n)$ and allows for updates in $O(log n)$ as well, compared to the prefix sum array that allows $O(1)$ queries but $O(n)$ updates.

\subsubsection{Segment Tree}

Segment Trees are conceptually similar to Fenwick Trees, in that they support range queries in $O(log n)$ and updates in $O(log n)$ as well. Segment Trees have the notable benefit of supporting more functions that summation, though -- sums, products, maximum, minimum, GCD, LCM, as some examples.

The cost of this is some slight overhead -- there is more code to them than Fenwick Trees, and they require some additional memory as well, but generally most problems that can be solved with Fenwick Trees can also be solved with Segment Trees.

\subsubsection{Wavelet Tree}

Wavelet Trees are range based structures that lend themselves to several different operations -- namely, counting the number of occurrences of a value in a range, the number of occurrences of between two values within a range, and getting the $k$th smallest value in a range.

Construction of the Wavelet Tree can be done as...

Counting the occurrences of a value in a range can be done as...

The number of occurrences of any number between two values can be done as...

The $k$th smallest value in a range can be implemented as...

\subsection{Ropes}

\section{Algorithm Analysis}
\index{Algorithm Analysis}

\subsection{Big-O Notation}
\index{Big-O Notation}

Under Construction

\subsection{Runtime Complexity}
\index{Runtime Complexity}

The main application of Big-O notation is to describe runtimes of algorithms.

While we will say things like "$O(n)$ algorithms will take $n$ operations" this is not entirely true, and is a generalization. If you were to calculate out how many operations an algorithm takes, Big-O only concerns itself with the largest term. For example, if you had an algorithm that took $n^2 + 100n$ operations, you would have $O(n^2)$ even though the $100n$ is significant. $O(n^2)$ could be $999n^2$ operations or it could be $0.1n^2 + 10^9n + 10^{100}$, since Big-O is only a measure of the highest growing term without any coefficients.

There are many frequent examples of algorithm complexities:

\begin{itemize}
\item $O(1)$ means that an algorithm is done in constant time, no matter how big the input is. A trivial example would be "for a list of $n$ elements, print the first element" because the algorithm is not concerned with how many elements there are after the first one.
\item $O(log n)$
\item $O(n)$ is an extremely common runtime, where you have to go through every element of the input.
\item $O(n log n)$
\item $O(n^2)$ is a very common runtime that can be best described as "for every input, go through every input."
\item $O(n^3)$ is much like $O(n^2)$ except that it is "for every pair of input, go through every input" where every pair of input is $O(n^2)$ itself.
\end{itemize}

To elaborate a bit on what these actually mean, we should look at some example programs. For a $O(1)$ algorithm, we can look at:

\inputpython{code/runtime_complexity_1.py}

Notice that it doesn't matter if $n$ is extremely large or extremely small, the program will take the same amount of time to run in any case. Compare this to a $O(n)$ algorithm:

\inputpython{code/runtime_complexity_n.py}

Where a small $n$ will run very fast, but a large $n$ might take a long time to run. This can then be compared to a $O(n^2)$ algorithm:

\inputpython{code/runtime_complexity_n2.py}

If a $O(n)$ algorithm took a long time with a large $n$, this algorithm will take an extremely long time. Specifically, if $n$ is $10,000$, then a $O(n)$ algorithm will take $10,000$ operations while a $O(n^2)$ algorithm will take $10,000^2 = 100,000,000$ operations. Fortunately, $10,000$ is not a huge amount of operations for a computer, and usually (at least for most online judges) it's around $10^8$ that a $O(n)$ algorithm is still relatively safe to use for a time limit of 1 second. When $n$ is $10^8$ though, a $O(n^2)$ algorithm would perform $10^{16}$ operations which is far beyond 1 second of runtime.

You might be able to notice a pattern, where every nested for-loop from $0$ to $n$ multiplies our Big-O by $n$ as well. Indeed, if we had 3 nested for-loops we would have $O(n^3)$. 4 nested for-loops from $[0,n)$ would be $O(n^4)$. And so on.

There are several less common time complexities as well, but some of the uncommon (but far from unheardof) runtimes would be:

\begin{itemize}
\item $O(\sqrt{n})$
\item $O(n log log n)$
\item $O(2^n)$
\item $O(n!)$
\item $O(n^n)$
\end{itemize}

As well, there are sometimes time complexities that rely on multiple variables. For example, you will see:

\begin{itemize}
\item $O(v + e)$
\item $O(v * e)$
\end{itemize}

\subsection{Space Complexity}
\index{Space Complexity}

Space complexity is comparable to runtime complexity. But instead of measuring how much time an algorithm takes to run, it measures how much additional data the algorithm needs to store.

If you have a problem with an input of size $n$, and you need to store elements into an array of size $n$ as well, we are talking about $O(n)$ space complexity. If you have a 2 dimensional array where both dimensions are size $n$ (in other words, we're storing $n^2$ individual elements), our space complexity is $O(n^2)$. If we never have to store any additional data (or more accurately, the amount of data we store doesn't change depending on the problem size) we're dealing with $O(1)$.

In general, all the same complexities that exist with runtime complexities are possible, but we are less likely to run into the less common ones, and likewise we are less likely to concern ourselves with the memory complexity (more often than not, runtime constraints are what limit you in a problem, memory constraints are not uncommon to be a problem but are less often than runtime).

\subsection{Precision}
\index{Precision}

Frequently, we will need to deal with precision in solving problems. Specifically, often we have problems where larger inputs do not fit into smaller types (for integers), or repeated arithmetic makes us require a certain level of precision that smaller types don't offer (for floats).

\subsubsection{Integer Types}
\index{Integer Types}

\index{Shorts} \index{Ints} \index{Longs} \index{Unsigned Types} \index{Arbitrary-Precision Integers}

\begin{itemize}
\item \textbf{16-bit signed} (\mintinline{cpp}{short} in C++ and Java) supports $[-2^{15},2^{15})$ and can hold integers up to over $10^4$.
\item \textbf{16-bit unsigned} (\mintinline{cpp}{unsigned short} in C++) supports $[0,2^{16})$ and can hold integers up to over $10^4$.
\item \textbf{32-bit signed} (\mintinline{cpp}{int} in C++ and Java) supports $[-2^{31},2^{31})$ and can hold integers up to over $10^9$.
\item \textbf{32-bit unsigned} (\mintinline{cpp}{unsigned int} in C++) supports $[0,2^{32})$ and can hold integers up to over $10^9$.
\item \textbf{64-bit signed} (\mintinline{cpp}{long long} in C++ and just \mintinline{java}{long} Java) supports $[-2^{63},2^{63})$ and can hold integers up to over $10^{18}$.
\item \textbf{64-bit unsigned} (\mintinline{cpp}{unsigned long long} in C++) supports $[0,2^{64})$ and can hold integers up to over $10^{19}$.
\item \textbf{128-bit signed} (\mintinline{cpp}{__int128} in GCC C++) supports $[-2^{127},2^{127})$ and can hold integers up to over $10^{38}$.
\item \textbf{128-bit unsigned} (\mintinline{cpp}{unsigned __int128} in GCC C++) supports $[0,2^{128})$ and can hold integers up to over $10^{38}$.
\item Python's \textbf{int} will internally convert into an arbitrary-precision data type when it needs to in order to store integers of any size (restricted only by memory).
\item Java's \textbf{BigInteger} is much like Python's \textbf{int}. It does have its own features, such as supporting a method \mintinline{java}{isProbablePrime(10)} that can tell you (to a likelihood of $1 - \frac{1}{1^{10}}$) whether the number is prime or not (though it will not give false negatives, if it returns \mintinline{java}{false} is is definitely not prime, but returning \mintinline{java}{true} only means that it is probably prime).
\end{itemize}

It's notable to mention that C++ does not have its own arbitrary-precision integer type. There are popular third-party libraries for this, but those aren't normally available in contests or on online judges. It's not uncommon for competitive programmers who use C++ to write their own arbitrary-precision integer type (you can find examples in many teams' ICPC notebooks) but it is also common for problems to not require arbitrary precision types.

\subsubsection{Floating Point Types}
\index{Floating Point Types}

\index{Floats} \index{Doubles} \index{Arbitrary-Precision Floats}

\begin{itemize}
\item \textbf{32-bit floating point} (\mintinline{cpp}{float} in C++ and Java) supports up to $\pm3.40282 * 10^{38}$.
\item \textbf{64-bit floating point} (\mintinline{cpp}{double} in C++ and Java or \mintinline{python}{float} in Python, the normal floating point representation) supports up to $\pm1.79769 * 10^{308}$.
\item \textbf{80-bit floating point} (\mintinline{cpp}{long double} in C++) supports up to $1.18973 * 10^{4932}$.
\item \textbf{128-bit floating point} (\mintinline{cpp}{__float128} in GCC C++) supports up to $\pm1.18973 * 10^{4932}$ but with greater precision than \textbf{80-bit} floats.
\item Python's \textbf{decimal} is a library class that allows you to set precision as you need, via setting \mintinline{python}{getcontext().prec} (used as \mintinline{python}{getcontext().prec = 50} for 50 significant places).
\item Java's \textbf{BigDecimal} is like Python's \textbf{decimal} in that you can set your own precision, by calling \mintinline{java}{.setScale(50)} on the \textbf{BigDecimal} object.
\end{itemize}

Like with arbitrary-precision integers, C++ doesn't offer any standard library solution for arbitrary-precision floats. This is generally less of an issue because problems that can't be solved with \mintinline{cpp}{long double} or \mintinline{cpp}{__float128} and actually require arbitrary-precision are quite rare (unlike integers, it is extremely rare to find arbitrary-precision floats in ICPC team notebooks).

Floating point precision mostly comes into play when we're dealing with huge amounts of repeated operations. To demonstrate, we can simulate the difference between a few of these types (in C++) with something like:

\inputcpp{code/runtime_float_precision.cpp}

Which, when we change what floating point type we're using, gets us:

\begin{itemize}
\item \mintinline{cpp}{float} is $10020363264.0000000000$ which is $\sim20363264$ off the correct answer.
\item \mintinline{cpp}{double} is $9999999999.8307151794$ which is $\sim0.1692848206$ off the correct answer.
\item \mintinline{cpp}{long double} is $9999999999.9999834169$ which is $\sim0.0000165831$ off the correct answer.
\item \mintinline{cpp}{__float128} is $10000000000.0000000000$ which is accurate to the precision we're using.
\end{itemize}

In general, \mintinline{cpp}{float} isn't very accurate and has trouble handling repeated arithmetic like this -- in a problem that requires lots of repeated arithmetic a simple \mintinline{cpp}{float} might not be sufficient. \mintinline{cpp}{double} is a lot better and has much greater precision, but is still not perfect and may occasionally have precision issues. \mintinline{cpp}{long double} is better and reduces the imprecision from doubles even further, but it still is not entirely precise with repeated arithmetic.

\mintinline{cpp}{__float128} gives us the best precision out of all of them, but (as mentioned) is not guaranteed to work in all compilers since it's a GCC extension. In addition, \mintinline{cpp}{cout << val} doesn't work with \mintinline{cpp}{__float128} and it is necessary to cast it to another type, such as \mintinline{cpp}{cout << (double)val}. Because the issue is the precision when doing lots of arithmetic that eventually results in significant precision losses, it's fine to cast it to a smaller type after performing that repeated arithmetic.

In terms of speed, \mintinline{cpp}{float} is the fastest, \mintinline{cpp}{double} is slightly slower (expect a slowdown of around 20-30\%), \mintinline{cpp}{long double} is slightly slower than that, and then \mintinline{cpp}{__float128} can be significantly slower (quick benchmarks indicate that $\sim15$ times slower than \mintinline{cpp}{double} is reasonable to expect).

Some considerations for Java and Python are that you have less options for your types than C++, but one of the options is an arbitrary precision type that can be as large as need be (that C++'s standard library doesn't offer). Note that arbitrary precision data types are relatively slow, so if possible it is recommended to use the primitive types.

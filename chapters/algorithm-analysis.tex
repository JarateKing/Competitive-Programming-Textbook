\section{Algorithm Analysis}
\subsection{Big-O Notation}



\subsection{Runtime Complexity}

The main application of Big-O notation is to describe runtimes of algorithms.

There are many frequent examples of algorithm complexities:

\begin{itemize}
\item $O(1)$ means that an algorithm is done in constant time, no matter how big the input is. A trivial example would be "for a list of $n$ elements, print the first element" because the algorithm is not concerned with how many elements there are after the first one.
\item $O(log n)$
\item $O(n)$ is an extremely common runtime, where you have to go through every element of the input.
\item $O(n log n)$
\item $O(n^2)$ is a very common runtime that can be best described as "for every input, go through every input."
\item $O(n^3)$ is much like $O(n^2)$ except that it is "for every pair of input, go through every input" where every pair of input is $O(n^2)$ itself.
\end{itemize}

There are several less common time complexities as well, but some of the uncommon (but far from unheardof) runtimes would be:

\begin{itemize}
\item $O(\sqrt{n})$
\item $O(n log log n)$
\item $O(2^n)$
\item $O(n!)$
\item $O(n^n)$
\end{itemize}

As well, there are sometimes time complexities that rely on multiple variables. For example, you will see:

\begin{itemize}
\item $O(v + e)$
\item $O(v * e)$
\end{itemize}

\subsection{Space Complexity}

Space complexity is comparable to runtime complexity. But instead of measuring how much time an algorithm takes to run, it measures how much additional data the algorithm needs to store.

If you have a problem with an input of size $n$, and you need to store elements into an array of size $n$ as well, we are talking about $O(n)$ space complexity. If you have a 2 dimensional array where both dimensions are size $n$ (in other words, we're storing $n^2$ individual elements), our space complexity is $O(n^2)$. If we never have to store any additional data (or more accurately, the amount of data we store doesn't change depending on the problem size) we're dealing with $O(1)$.

In general, all the same complexities that exist with runtime complexities are possible, but we are less likely to run into the less common ones, and likewise we are less likely to concern ourselves with the memory complexity (more often than not, runtime constraints are what limit you in a problem, memory constraints are not uncommon to be a problem but are less often than runtime).

\subsection{Precision}
\subsubsection{Integer Types}
\subsubsection{Floating Point Types}
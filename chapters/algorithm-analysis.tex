\section{Algorithm Analysis}
\subsection{Big-O Notation}



\subsection{Runtime Complexity}

The main application of Big-O notation is to describe runtimes of algorithms.

While we will say things like "$O(n)$ algorithms will take $n$ operations" this is not entirely true, and is a generalization. If you were to calculate out how many operations an algorithm takes, Big-O only concerns itself with the largest term. For example, if you had an algorithm that took $n^2 + 100n$ operations, you would have $O(n^2)$ even though the $100n$ is significant. $O(n^2)$ could be $999n^2$ operations or it could be $0.1n^2 + 10^9n + 10^{100}$, since Big-O is only a measure of the highest growing term without any coefficients.

There are many frequent examples of algorithm complexities:

\begin{itemize}
\item $O(1)$ means that an algorithm is done in constant time, no matter how big the input is. A trivial example would be "for a list of $n$ elements, print the first element" because the algorithm is not concerned with how many elements there are after the first one.
\item $O(log n)$
\item $O(n)$ is an extremely common runtime, where you have to go through every element of the input.
\item $O(n log n)$
\item $O(n^2)$ is a very common runtime that can be best described as "for every input, go through every input."
\item $O(n^3)$ is much like $O(n^2)$ except that it is "for every pair of input, go through every input" where every pair of input is $O(n^2)$ itself.
\end{itemize}

To elaborate a bit on what these actually mean, we should look at some example programs. For a $O(1)$ algorithm, we can look at:

\inputpython{code/runtime_complexity_1.py}

Notice that it doesn't matter if $n$ is extremely large or extremely small, the program will take the same amount of time to run in any case. Compare this to a $O(n)$ algorithm:

\inputpython{code/runtime_complexity_n.py}

Where a small $n$ will run very fast, but a large $n$ might take a long time to run. This can then be compared to a $O(n^2)$ algorithm:

\inputpython{code/runtime_complexity_n2.py}

If a $O(n)$ algorithm took a long time with a large $n$, this algorithm will take an extremely long time. Specifically, if $n$ is $10,000$, then a $O(n)$ algorithm will take $10,000$ operations while a $O(n^2)$ algorithm will take $10,000^2 = 100,000,000$ operations. Fortunately, $10,000$ is not a huge amount of operations for a computer, and usually (at least for most online judges) it's around $10^8$ that a $O(n)$ algorithm is still relatively safe to use for a time limit of 1 second. When $n$ is $10^8$ though, a $O(n^2)$ algorithm would perform $10^{16}$ operations which is far beyond 1 second of runtime.

You might be able to notice a pattern, where every nested for-loop from $0$ to $n$ multiplies our Big-O by $n$ as well. Indeed, if we had 3 nested for-loops we would have $O(n^3)$. 4 nested for-loops from $[0,n)$ would be $O(n^4)$. And so on.

There are several less common time complexities as well, but some of the uncommon (but far from unheardof) runtimes would be:

\begin{itemize}
\item $O(\sqrt{n})$
\item $O(n log log n)$
\item $O(2^n)$
\item $O(n!)$
\item $O(n^n)$
\end{itemize}

As well, there are sometimes time complexities that rely on multiple variables. For example, you will see:

\begin{itemize}
\item $O(v + e)$
\item $O(v * e)$
\end{itemize}

\subsection{Space Complexity}

Space complexity is comparable to runtime complexity. But instead of measuring how much time an algorithm takes to run, it measures how much additional data the algorithm needs to store.

If you have a problem with an input of size $n$, and you need to store elements into an array of size $n$ as well, we are talking about $O(n)$ space complexity. If you have a 2 dimensional array where both dimensions are size $n$ (in other words, we're storing $n^2$ individual elements), our space complexity is $O(n^2)$. If we never have to store any additional data (or more accurately, the amount of data we store doesn't change depending on the problem size) we're dealing with $O(1)$.

In general, all the same complexities that exist with runtime complexities are possible, but we are less likely to run into the less common ones, and likewise we are less likely to concern ourselves with the memory complexity (more often than not, runtime constraints are what limit you in a problem, memory constraints are not uncommon to be a problem but are less often than runtime).

\subsection{Precision}

Frequently, we will need to deal with precision in solving problems. Specifically, often we have problems where larger inputs do not fit into smaller types (for integers), or repeated arithmetic makes us require a certain level of precision that smaller types don't offer (for floats).

\subsubsection{Integer Types}

\begin{itemize}
\item \textbf{16-bit signed} (\mintinline{cpp}{short} in C++ and Java)
\item \textbf{16-bit unsigned} (\mintinline{cpp}{unsigned short} in C++)
\item \textbf{32-bit signed} (\mintinline{cpp}{int} in C++ and Java)
\item \textbf{32-bit unsigned} (\mintinline{cpp}{unsigned int} in C++)
\item \textbf{64-bit signed} (\mintinline{cpp}{long long} in C++ and just \mintinline{java}{long} Java)
\item \textbf{64-bit unsigned} (\mintinline{cpp}{unsigned long long} in C++)
\item \textbf{128-bit signed} (\mintinline{cpp}{__int128} in GCC C++)
\item \textbf{128-bit unsigned} (\mintinline{cpp}{unsigned __int128} in GCC C++)
\item Python's \textbf{int} 
\item Java's \textbf{BigInteger}
\end{itemize}

\subsubsection{Floating Point Types}

\begin{itemize}
\item \textbf{32-bit floating point} (\mintinline{cpp}{float} in C++ and Java)
\item \textbf{64-bit floating point} (\mintinline{cpp}{double} in C++ and Java)
\item \textbf{80-bit floating point} (\mintinline{cpp}{long double} in C++)
\item \textbf{128-bit floating point} (\mintinline{cpp}{__float128} in GCC C++)
\item Python's \textbf{float}
\item Java's \textbf{BigDecimal}
\end{itemize}

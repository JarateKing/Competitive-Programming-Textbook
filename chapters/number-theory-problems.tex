\section{Number Theory Problems}
\index{Number Theory}

\subsection{Binary Exponentiation}
\index{Binary Exponentiation}

Consider the naive way to calculate $a^n$ (where $n$ is an integer), we simply get the product of $n$ $a$s. In terms of code, we can simply initialize a variable to $1$ and then have a for-loop that multiplies our variable by $a$ a total of $n$ times. This works fine for $0^n$ where our answer should be 0, and $a^0$ where our answer should be $1$, but doesn't work for $0^0$ which is undefined (for simplicity's sake, we'll ignore $0^0$).

\inputcpp{code/number_theory/pow_naive.cpp}

Unfortunately, this is relatively slow. In many problems or algorithms where you need to use exponentiation, calculating it in $O(n)$ is far too slow.

Binary exponentiation is the idea of calculating an arbitrary integer power as a product of different values of $a^{2^n}$. Specifically, we rely on two properties:
\begin{itemize}
\item We can represent a number as a sum of powers of 2 based on its binary representation \index{Binary Representation}. Consider the decimal number $10$, which has a binary representation of $1010_2$. We can tell that $10 = 2^1 + 2^3$ from its binary representation.
\item $a^{n+m} = a^n * a^m$. Rather than calculate the larger $a^{n+m}$ we can instead calculate it as the product of smaller powers that add to $n+m$.
\end{itemize}

When we combine these together, we can notice that $a^{10} = a^{2^1 + 2^3} = a^{2^1} * a^{2^3}$. What makes calculating this very fast is noticing that $a^{2^{i+1}} = (a^{2^i})^2$, so that we can quickly iterate through values of $a^{2^i}$:

\inputcpp{code/number_theory/pow_binary.cpp}

The value in this is that we have significantly reduced our runtime. Rather than calculate in $O(n)$, we calculate in $O(log_2n)$. Now, rather than $2^{2^{31}-1}$ requiring over 2 billion iterations of a for-loop in the naive method, it takes a mere 31 iterations of a while-loop in our binary exponentiation method.

\subsection{Primes}
\index{Prime Numbers}

Prime numbers are a recurring concept inside number theory, and is foundational to a lot of more advanced applications.

In short, prime numbers are integers $> 1$ that cannot be evenly divided by any other integer $> 1$. In other words, a prime number is a number that can't be represented as the product of more than one prime number. For example, $2, 3, 5, 7, 11, 13, 17, 19, 23, ...$ are prime numbers, because none of the following can be representated as the product of other prime numbers.

The opposite of a prime number is a composite number, that is evenly dividable by some number of prime numbers. $4 = 2^2, 6 = 2*3, 8 = 2^3, 9 = 3^2, 10 = 2*5, 12 = 2^2*3, ...$ are examples of composite numbers. Every composite number has exactly one representation as the product of prime numbers.

\subsubsection{Prime Sieve}
\index{Prime Sieve} \index{Sieve of Eratosthenes}

Under Construction

\subsubsection{Miller-Rabin Primality Test}
\index{Miller-Rabin} \index{Primality Test}

Under Construction

\subsubsection{Prime Factoring}
\index{Prime Factoring}

We can factorize a number into its prime factors by simply iterating over every possible divisor and attempting to divide it. As long as we start with the smallest possible divisors (with 2 being the smallest nontrivial divisor) and work our way up to larger divisors, we are guaranteed to only find prime numbers (since any composite number would've already been handled by its prime factors).

A very important optimization is to note that we only need to actually iterate up to $\sqrt{n}$ inclusive. We can realize that if a number $n$ is composite, then it must have at least one prime factor $\le \sqrt{n}$. In contrast, if a number has no prime factors $\le \sqrt{n}$, we can tell that it's prime. We can use this for our factoring algorithm -- we can find all prime factors $\le \sqrt{n}$ through attempting division, and then if what's left over of $n$ is not equal to 1 we know that is a prime as well.

\inputcpp{code/number_theory/prime_factorization_naive.cpp}

\subsubsection{Pollard's Rho Algorithm}
\index{Pollard's Rho Algorithm}

Under Construction

\subsection{Greatest Common Divisor}
\index{Greatest Common Divisor}

The greatest common divisor between two non-zero integers is as its name suggest -- the largest number that can divide both of two given integers. For example the divisors of $12$ are $\{1,2,3,4,6,12\}$ and the divisors of $16$ are $\{1,2,4,8,16\}$, and as a result the GCD between the two is $4$ which is the largest shared divisor between the two. Similarly, the GCD of $5$ and $6$, which share no divisors other than $1$, is $1$. As well, it can be observed that the GCD of two equal numbers is that number itself again, since a number contains itself as a divisor (and as such, the GCD of $10$ and $10$ is $10$).

We also define the GCD of any number and 0 to be that number: $GCD(x,0) = x$.

GCDs have many applications. One of the simplest being in reducing fractions -- we know by looking at it that $\frac{2}{6}$ can be simplified into $\frac{1}{3}$. We can easily obtain this simplified fraction by dividing both the numerator and the denominator by the GCD of the numberator and denominator. For example, we can simplify the fraction $\frac{1800}{2100}$ by finding the GCD between both parts of the fraction, which is 300, and dividing both $1800$ and $2100$ by it. We get $\frac{6}{7}$ which is the most reduced form that fraction can take.

\subsubsection{Basic GCD}
\index{Euclidean Algorithm}

Calculating the GCD of two numbers can be done through a variety of methods. We'll explore the \textit{Euclidean Algorithm} approach to the problem.

It's based on noticing that the GCD of two numbers doesn't change when you subtract the smaller number from the larger. For example, $GCD(100, 12) = 4$, as is $GCD(100-12, 12) = 4$. This can be performed repeatedly as well. For example, $GCD(100, 12) = GCD(88, 12) = GCD(76, 12) = ... = GCD(16, 12) = GCD(4, 12)$. We can continue the process even further by swapping $4$ and $12$ (since 12 is now larger), and find that $GCD(12, 4) = GCD(8, 4) = GCD(4, 4) = GCD(0, 4)$ before we can't go any further (we can keep subtracting 0 from 4 but it doesn't change the values we have, so when one parameter is 0 we have our base case).

As it turns out, if you repeat this process enough you will always be left with $GCD(x, 0)$. And calculating the GCD of any value and $0$ is easy, it's that value unchanged. So we have our GCD now.

We can convert this into a form that's easier to calculate. Repeated subtraction is relatively slow when we're dealing with large values, but we may notice that repeatedly replacing $a$ with $a - b$ until $a < b$ is equivalent to finding $a \% b$ which we can calculate in a single operation. We may also notice that if $a < b$ then $a \% b = a$, which we can use to avoid swapping values if one is larger than the other and instead keep swapping each iteration. This prevents us from having to deal with our second parameter being larger than the first, or our first parameter being 0 to start with. 

This is fairly easy to express recursively. All together, our code may look like:

\inputcpp{code/number_theory/gcd_euclid.cpp}

However, you generally don't need to code this yourself. GCC comes with \mintinline{c++}{__gcd(a,b)} as a function, C++17 added \mintinline{c++}{std::gcd(a,b)} to the standard library, Python includes \mintinline{python}{math.gcd(a,b)} through its math module, and Java does not offer a GCD function for primitives but does support \mintinline{java}{a.gcd(b)} for its \mintinline{java}{BigInteger} class.

\subsubsection{Extended GCD}
\index{Extended GCD} \index{Extended Euclidean Algorithm}

Sometimes we want to know more than just the GCD of two numbers -- namely we might want to know what values of $x$ and $y$ could satisfy the equation $ax + by = GCD(a,b)$ where $a$ and $b$ are known.

The values of $x$ and $y$ that can satisfy that equation aren't necessarily unique. For example, with $a = b = 1$ which has a GCD of $1$, we can find that $x = 1, y = 0$ works, or $x = 2, y = -1$ works, or $x = -10, y = 11$, or infinitely many different other options. For our purposes, we're interested in finding one valid solution.

\inputcpp{code/number_theory/gcd_extended.cpp}

\subsubsection{Primal Representation}
\index{Primal Representation}

Given a prime factorization of two numbers, the GCD is the product of factors in common. For example, the prime factorization of $60$ is $\{2,2,3,5\}$ while the prime factorization of $24$ is $\{2,2,2,3\}$. The factors in common are $\{2,2,3\}$ whose product is 12, which is the GCD of 60 and 24.

\subsubsection{Least Common Multiple}
\index{Least Common Multiple}

The least common multiple between two non-zero integers can be calculated quickly: $\frac{a * b}{gcd(a, b)}$. Since $gcd(a, b)$ is a divisor of both $a$ and $b$ (and by extension $a * b$) this will always be an integer as well.

The primal representation of the LCM is similar to the primal representation of their GCD, except it's the product of factors they don't share multiplied by the product of factors in common (or GCD). For example, with $60$ and $24$ again, we have $\{2,5\}$ as unshared factors, along with $\{2,2,3\}$ as common factors, which is $2 * 2 * 2 * 3 * 5 = 120$ which is the LCM.

\subsubsection{Coprime Numbers}
\index{Coprime Numbers} \index{Relatively Prime}

Numbers are considered coprime (or relatively prime) if their GCD is 1. That is to say, they have no prime factors in common.

Prime numbers are, by necessity, coprime to other primes. This is because they would share no prime factors (for both, their sole prime factor being themselves) and as such have a GCD of 1. Following similar logic, any number is either coprime to a prime, or it's a multiple of that prime (because the only scenario that the GCD wouldn't be 1 is when the prime is a factor).

\subsection{Modular Arithmetic}
\index{Modular Arithmetic}

\subsubsection{Addition}

Addition can be fairly simple -- we add our numbers together and then perform the modulo operation on it. For example:

\inputcpp{code/number_theory/modulo_add.cpp}

The modulo operation is relatively expensive for a basic arithmetic operation, however. If we can guarantee that $a$ and $b$ are both less than $m$, we can avoid doing any modulo operations with:

\inputcpp{code/number_theory/modulo_add_fast.cpp}

\subsubsection{Subtraction}

Subtraction is similar to addition, but we need to guarantee that we don't go negative. We can fix this by simply adding $m$ before performing our modulo operation:

\inputcpp{code/number_theory/modulo_sub.cpp}

Likewise with addition, we can also optimize our subtraction function to not perform any modulo operation:

\inputcpp{code/number_theory/modulo_sub_fast.cpp}

\subsubsection{Multiplication}

Multiplication can (mostly) be performed similar to the non-optimized method of addition and subtraction -- by simply doing the multiplication operation and then performing the modulo operation on that. However, we may be concerned with overflows.

Specifically, consider $m = 10^9 + 7$. The largest numbers we will multiply will be when both $a$ and $b$ are equal to $10^9 + 6$. $a * b$ is somewhat above $10^{18}$ which is outside of the range of what a 32-bit integer can hold. In a language like C++ or Java, we will have to ensure that we use 64-bit integers when performing our multiplication:

\inputcpp{code/number_theory/modulo_mul.cpp}

\subsubsection{Exponentiation}

Exponentiation follows a very similar methodology to the binary exponentiation method we explored earlier, except now at each multiplication we perform, we need to instead perform the multiplication modulo $m$. We can avoid our \mintinline{cpp}{modulo_mul} method's repeated casting to 64-bit integers by simply keeping every value we multiply in 64-bit integers always.

\inputcpp{code/number_theory/modulo_pow.cpp}

The above code also handles a special edge case where $m = 1$, and by consequence our answer will always be $0$.

\subsubsection{Division (Prime Modulo)}

So far we've explored and implemented operations that seem fairly similar to how they'd be implemented without modulus -- addition, subtraction, and multiplication are the same as normal but with the modulus operation at the end of its computation. Exponentiation is similar, except with modulus operations performed at intermediary steps as well.

Division is more complicated. While some operations, such as $\frac{6}{2} \% 7$ can be expressed as the regular division operation (since 2 evenly divides 6, and equals the integer 3) modulo 7, the same is not true of $\frac{2}{6} \% 7$. $\frac{2}{6} = \frac{1}{3} = 0.333...$ but modular arithmetic is based on integers, so this won't do.

Let's consider what division really is: if multiplication is finding $x * y = z$ with known values of $x$ and $y$, division is finding $x * z = y$. Modular division follows the same principle, where we're trying to find $(x * z) \% m = y \% m$ given known values of $x$, $y$, and $m$.

If we work it out by hand, we can discover that a solution for $\frac{2}{6} \% 7$ is $5$ -- by rearranging it into our multiplication equation, we find that $(6 * 5) \% 7 = 2 \% 7$ to be valid.

A naive method of calculating modular division would be to try every possible value $[0, m)$ and see if it satisfies the above equation. Unfortunately, when our $m$ is large as it often is, this can be far too slow to reliably use.

This section is about division with a prime $m$ because it's easier to do (we'll cover arbitrary values of $m$ later). Another way of representing division is as $x * \frac{1}{y} = z$ with known $x$ and $y$, and the same applies under modulus assuming that we can calculate $\frac{1}{y} \% m$.

As it turns out, it's not very difficult to calculate $\frac{1}{y} \% m$ when $m$ is prime. Fermat's little theorem states that $a^m \% m = a \% m$ when $m$ is prime, which is equivalent to $a^{m-1} \% m = 1 \% m$, and $a$ can be divided out from both sides to obtain $a^{m-2} \% m = \frac{1}{a} \% m$. As such, as long as we know $m$ is prime, we can quickly calculate $\frac{1}{y}$ by raising it to the (modular) power of $m-2$. All together, we can calculate $\frac{x}{y} \% m$ as $x * y^{m-2} \% m$.

\subsubsection{Modular Inverse}
\index{Modular Inverse}

Adding on from the section on division (prime modulo), when we try to find $\frac{1}{a}$ (equivalently $a^{-1}$), it turns out there's a name for this. It's the \textit{modular multiplicative inverse} or just \textit{modular inverse}. And it isn't limited to just primes either, there can be 

That said, we aren't guaranteed for a multiplicative inverse of a number to exist under every modulo. Namely, for there to be a multiplicative inverse of $a$ under modulo $m$, $a$ and $m$ need to be coprime. This will always be the case when $m$ is prime (since $a \% m$ is smaller than $m$ and cannot be a multiple of $m$, and as a prime number $m$ is coprime to anything that isn't a multiple of it), but may not be the case if $m$ is composite. For example, $a = 2, m = 4$ has no multiplicative inverse, since $2$ and $4$ aren't coprime. We can also verify this by trying to find an integer solution for $(2 * x) \% 4 = 1 \% 4$ which we can quickly find does not exist (for all values of $x$ we will either obtain $2 \% 4$ or $0 \% 4$, never $1 \% 4$).

As for calculating the modular inverse, recall the motivation for the extended GCD, finding $x$, $y$, and $GCD(a,b)$ in the equation $ax + by = GCD(a,b)$ given $a$ and $b$. Now consider finding the GCD of $a$ and $m$, knowing that they are coprime: $ax + my = 1$ since the GCD of any coprime numbers is 1. We can quickly rearrange this to find $ax = 1 - my$, and modulo $m$ we find $ax \% m = 1 \% m$. If we consider that this can be further rearranged into $x \% m = \frac{1}{a} \% m$, we've successfully calculated our modular inverse.

Conveniently, our function to determine whether $a$ and $m$ are coprime and our function to calculate the value of $x$ are actually the same function. This makes the code for the modular inverse relatively short:

\inputcpp{code/number_theory/modulo_inverse.cpp}

\subsubsection{Division (Any Modulo)}

Division under any modulo can now be implemented relatively easily: replace the prime mod specific method of finding the modular inverse $a^{m-2}\%m$ and use the general modular inverse method instead. We also have to take care that the value of this modular inverse isn't $-1$ (or whatever other value we have set as invalid), but the core idea of the function remains the same.

\subsubsection{Discrete Log}
\index{Discrete Log}

Under Construction

\subsubsection{Square Root}

Under Construction

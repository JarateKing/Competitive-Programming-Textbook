\section{Foundational Algorithms}
\index{Algorithms}

There are a few algorithms that should be covered and discussed before going into much further explorations.

\subsection{Sorting}
\index{Sorting}

Sorting is likely the most fundamental algorithm you will use in competitive programming. It appears in many problems, to the point where problems that just require sorting to solve them are usually considered relatively easy. More often you'll see sorting be involved as a necessary component to more difficult problems. In many cases, sorting is required in order to perform efficient algorithms (as we will see soon, sorting is required to perform a \textbf{binary search} which is generally much faster than a \textbf{sequential search}).

The goal of a sorting algorithm is simple, to transform an array of data to be sorted. A basic example would be given $1,5,3,4,2$ as an array, sorting would yield $1,2,3,4,5$. Or, $1000,50,100,1$ becomes $1,50,100,1000$.

\subsubsection{Sorting Concepts}

A \textbf{sorted} array refers to one where elements are ordered according to some criteria. In practice, this is usually in ascending order \index{Ascending Order} (so that each element is larger than the previous) and less occasionally in descending order \index{Descending Order} (where each element is smaller than the previous, which sometimes is called "sorted in reverse").

In comparison, an \textbf{unsorted} array refers to one where this criteria is not met.

There are two main types of sorting algorithms: \textbf{comparison sorts}\index{Comparison Sorts} and \textbf{integer sorts}\index{Integer Sorts}. The comparison variety relies on exactly what it says, comparisons.

What determines ascending or descending order can depend on what specifically is being sorted. For integers it's easy, one is clearly larger or smaller (or equal) to another. Same with floating point numbers. For strings, it's usually lexicographical order (alphabetical order). For more complex types it's not necessarily clear. To sort an array of queues stacks, it's less obvious what you should do, and it may depend entirely on your use case. In some problems you'll even find yourself with a \mintinline{cpp}{pair<int,int>} where you want to sort by the first integer at one part of your program, and then later sort by the second integer. In a comparison sort, you will have some \textbf{comparison function} that determines how to compare different elements with each other.

An integer sort on the other hand doesn't make use of any comparison function, and instead relies on properties about integers themselves. As a result, it doesn't work on arbitrary types, and you can't make it have its own sorting criteria by changing how it should compare elements. Generally, these sorting algorithms are much more specialized. That said a few integer sorting algorithms can offer very good runtime complexity, and depending on the data can offer significant speedups over any comparison sort alternative.

Another classification for sorting algorithms is whether they're \textbf{stable} \index{Stable Sorting Algorithms} or not. A stable sorting algorithm will ensure that different elements that compare equally to eachother (for custom classes with their own comparators for example) will have keep the relative order that they appeared in originally. For example, with an array of pairs $\{1,1\}, \{2,2\}, \{1,3\}, \{2,4\}, \{1,5\}$ where the comparison function only compares the first element of each pair, a stably sorted array would look like $\{1,1\}, \{1,3\}, \{1,5\}, \{2,2\}, \{2,4\}$.

In contrast, an \textbf{unstable} \index{Unstable Sorting Algorithms} algorithm makes no guarantees. It could pretty easily still end up sorted the exact same as a stable sorting algorithm, but it isn't necessarily going to.

If every element of the array is unique, the difference between stable and unstable doesn't matter. If there are duplicate elements in the array, but you don't have any other data associated with them (two duplicate elements will be identical to eachother) then it doesn't matter either. It also wouldn't matter if duplicate elements with different data exist, but you don't really care what order they appear in as long as they're sorted relative to other elements. It's only in the (relatively narrow) circumstance that all these properties hold and the original order of duplicate elements is relevant that the difference between stable and unstable matters.

\subsubsection{Insertion Sort}
\index{Insertion Sort}

\textbf{Insertion sort} is a stable comparison-based sorting algorithm that's among the simplest to implement. The algorithm consists of iterating through the unsorted array, and for each element we check if the element before it is larger and swap if so, then check if the element before that is larger and swap if so, etc. In other words, we keep shifting an element earlier until elements to the left are all smaller and elements to the right are all larger.

\inputcpp{code/algorithms/insertion_sort.cpp}

What is the run-time of this algorithm? Unfortunately it is a fairly naive algorithm, and runs in $O(n^2)$. The worst-case is pretty simple to create, when the array is reversed we will have to do the maximum number of comparisons and swaps. Insertion sort isn't recommended as a general-use algorithm for these reasons -- its runtime isn't ideal and its worst-case is very common to encounter (some test cases for a problem might be in reverse order just to cover all bases, not even necessarily to specifically exclude insertion sort).

While insertion-sort is generally not common because its $O(n^2)$ time complexity is too slow for large arrays, it does have very little overhead when dealing with small arrays to the point of often being one of the fastest sorting algorithms. Specifically, when we're dealing with elements in the CPU cache, swapping elements is extremely fast. For this reason it's not uncommon to see it beind used as part of a more complicated hybrid sort. Timsort, the standard library sorting algorithm of Python, is one such hybrid sort that is based on a combination of insertion sort and merge sort, for example.

\subsubsection{Quicksort}
\index{Quicksort}

\subsubsection{Mergesort}
\index{Mergesort}

\subsubsection{Counting Sort}
\index{Counting Sort}

Consider the case where you have $10^9$ 1-digit numbers that you want to sort. It is certainly possible to perform a comparison-based sort that will run in $O(n log n)$, there is actually an integer sort that's perfectly suited for this sort of problem that runs in $O(n)$.

\textbf{Counting sort} is based around having an array where each element is the number of times that index appeared in the original unsorted list. For example, with the array $0,5,2,2,2,1,5,9$ we would have the counts $1,1,3,0,0,2,0,0,0,1$ because 0 appears once, 1 appears once, 2 appears three times, 4 never appears, and so on. Generating this is a simple process -- simply iterate through the unsorted list and increase the necessary count by 1 for every element.

The next step is to create a new sorted array, once we know the counts of each value. This time we iterate through our counts and insert them into our new array. When a value was counted multiple times, we insert it that many times. The result of this will be an array containing $0,1,2,2,2,5,5,9$, the sorted version of the original array.

\inputcpp{code/algorithms/counting_sort.cpp}

Looking at the concepts of sorting, we can note that counting sort is:
\begin{itemize}
\item An integer sort
\item $O(n+m)$ runtime, where $n$ is the array size and $m$ is the maximum element size (or size of the count array)
\item $O(m)$ auxillary memory is used
\end{itemize}

Note that our count array has to be as large as the maximum element. Because of this (and that we have to traverse the entire count array), this means counting sort is best suited for when the range of valid elements is very small. A huge amount of one-digit numbers is a great example of where counting sort does well (and was chosen as the example here for that reason). When our elements are potentially very large, counting sort is not very well suited.

\subsubsection{Cycle Sort}
\index{Cycle Sort} \index{Cycles}

Cycle sort is a sorting algorithm designed to minimize the number of times the array is modified. It has a slow runtime of $O(n^2)$ so it's not used for general purpose sorting (outside of on hardware where memory writes are extremely expensive compared to memory reads) but it does have the neat property that it will only ever move an element into its correct position. Whereas in quicksort or mergesort, a single element may end up getting swapped many times before it ends up in its sorted position, cycle sort will only ever move an element at most one time (zero times if it's already in the right spot, once if it needs to be moved). This makes it potentially useful for some tasks where we want to know \textit{how many elements need to be moved} or something along those lines, rather than just needing to sort it.

The core concept behind this algorithm is that moving an element from its unsorted position into a sorted position will also displace another element (for one element to be unsorted, there has to be at least one other element that's unsorted). Sometimes moving that second element into its correct position will displace a third element, which may itself displace another, and so on. Eventually we will end up trying to move an element into the spot our first element used to take up. We can consider these as cycles -- moving elements into their correct positions will form a cycle of some length where we have to move other elements afterwards. The cycle length is how many elements need to be moved before we end up at the position we started.

For example, if we try to sort $\{2,1\}$, we will find we have a cycle of length 2 -- when we put $2$ into its correct position, we displace $1$ and have to move that into its correct position (which used to belong to $2$ but is now open). Similarly with $\{2,3,1\}$, we have a cycle of length 3, and moving $2$ into its correct position displaces $3$ which would then displace $1$ before we finish the cycle. For simplicity's sake, we can consider a single element in its sorted position already to have a cycle length of 1, but we don't need to move it because it would simply return to the position it's already at.

The logic of the algorithm itself is to iterate through the array, and for each element, count the number of elements smaller than it (so that we know the correct position of that element in the sorted array). When we find an element that's out of place, we track its position as the start of the cycle, then attempt to place it in its proper position. We then try the same with the element we just displaced, counting the number of elements smaller than it and finding its correct position in the array, and putting it in its proper spot. If we bring it to the start of the cycle, we simply move our displaced element there. If it's not the start of the array, then we're displacing another element and we repeat the process. We continue until the cycle is done, and then iterate to the next element in the array.

\subsubsection{Other Sorting Algorithms}

It's likely that you've heard about or seen other sorting algorithms that we haven't covered in the above subsections. The truth is, we usually aren't too concerned with the details of various sorting algorithms in competitive programming. We listed some sorting algorithms because they're relevant (the details for merge-sort sometimes is useful to know for solving some problems), useful (counting sort is something that will occasionally get used for competitive programming), or more likely that they teach some fundamental concepts that we use elsewhere (quicksort's partitioning sees use in other algorithms).

If we're not too concerned with sorting algorithm details, what do we do instead? In the vast majority of cases, using the standard library to sort things is sufficient (and definitely far easier).
\begin{itemize}
\item C++ supports \mintinline{cpp}{std::sort} for general purpose sorting, and \mintinline{cpp}{std::stable_sort} for when a stable sort is required (because \mintinline{cpp}{std::sort} is not guaranteed to be stable). Both of these require \mintinline{cpp}{#include <algorithm>}.
\item Java supports \mintinline{java}{Arrays.sort()} via \mintinline{java}{import java.util.Arrays;} for arrays specifically, and \mintinline{java}{Collections.sort()} via \mintinline{java}{import java.util.Collections;} for general data structure containers (including arrays). These sorts are stable by default.
\item Python lets you either create a sorted copy of a list or iterable via \mintinline{python}{sorted(arr)} or lets you sort a list directly via \mintinline{python}{arr.sort()} (which is more efficient, but loses the original ordering and only works for lists). These sorts are guaranteed to be stable.
\end{itemize}

For interest, some other sorting algorithms are described here:
\begin{itemize}
\item \textbf{Bubble sort} \index{Bubble Sort} is a commonly taught algorithm as a very simple (but very naive and slow) sorting algorithm. You iterate through the list multiple times, and if two adjacent elements are out of order, they get swapped. It doesn't see much practical use because other sorting algorithms are generally better.
\item \textbf{Selection sort} \index{Selection Sort} is a naive sorting algorithm that for each element, it finds whatever element is smallest out of all elements to the right and swaps it into its right place. You can be guaranteed that, at the $i$th iteration, the range $[0,i)$ contains the right elements that need to be sorted.
\item \textbf{Heapsort} \index{Heapsort} is an algorithm that inserts all elements of an array into a heap, and then removes all those elements from the heap.
\item \textbf{Radix Sort} \index{Radix Sort} is an integer sorting algorithm that recursively sorts by individual digits -- you take either the most-significant-digit or least-significant-digit of a number, distribute elements into different buckets based on what that digit was, then within each bucket you repeat with the next digit, and so on.
\item \textbf{Bogosort} \index{Bogosort} an extremely naive algorithm that creates a random permutation of an array, checks if it's sorted, and if not repeats. It's notable that it has an expected runtime of $O(n!)$ and in the worst case (if you are extremely unlucky) will never finish, but in the best case is $O(n)$.
\item \textbf{Dropsort} \index{Dropsort} is an extremely fast algorithm that sorts an entire array in a single pass, by simply removing any elements that are smaller than the one before it. It is usually mentioned as a joke algorithm for this reason -- but the output is a sorted array in any case (even if it's potentially much smaller than the original array).
\end{itemize}

That isn't to say that we have a complete list of all sorting algorithms -- not at all, there are lots of them. Some fairly well-known but not listed here, some that are variants on existing algorithms, and some that are nearly unheard of but still a valid sorting algorithm nonetheless.

\hrulefill

\input{problems/large_and_small_pairs/description}

\subsection{Searching}
\index{Searching}

Often, you need to check whether an element exists within an array. Other times, you have to find the index of that element. In any case, given an array (or other list of elements), searching for an element is a common task. Like sorting, this is often fairly simple when done on its own, and more often than not it's a subtask of a larger problem or a part of a larger algorithm entirely.

There are various algorithms for searching for an element within an array.

\subsubsection{Sequential Search}
\index{Sequential Search} \index{Linear Search}

\textbf{Sequential search} (also called linear search) is the simplest form of searching for an element within an array. It consists of a simple for-loop that checks each individual element 

It's useful because it doesn't rely on the array being sorted or anything like that. While it is often quite slow, it can be useful when runtime is not a concern.

\subsubsection{Binary Search}
\index{Binary Search}

\textbf{Binary search} is an extremely common search algorithm that uses the structure of a sorted array to offer great speedups.

\subsubsection{Exponential Search}
\index{Exponential Search}

\textbf{Exponential Search} is a variant on binary search that both performs slightly better than binary search when the search key is near the beginning of the list, and also works for unbounded lists (not generally an issue with arrays, but if you were to binary search a nonstrictly increasing function without a clear maximum, this would apply).

The idea is to first obtain a valid range to binary search. Exponential search specifically tries to check if the search key potentially exists within the ranges $[0,1)$, $[1,2)$, $[2,4)$, $[4,8)$, $[8,16)$, and so on. The patterns for the ranges isn't too complicated -- we double our upper bound each time, and our lower bound is just our upper bound divided by 2. Checking whether the key is potentially within that range is simple -- as long as the key is between the upper and lower bounds of the range, it could exist.

Once we have the valid range, we simply perform a normal binary search on that range. The code as a result looks like:

\inputcpp{code/algorithms/exponential_search.cpp}

\subsubsection{Quickselect}
\index{Quickselect}

Let's consider the case where you want to find the $k$th smallest element in an unsorted array. One of the easiest options is to sort that array and then just look at the $k$th element there.

You will notice however, that we don't actually need most of the array to be sorted for this to be the case. If we're looking for the smallest element, it's no concern of ours if the majority of the array is unsorted. If we're looking for the median element, we don't care that what's to the left and what's to the right are fully sorted, just as long as we're sure the median element is where it's supposed to be (which we can verify by every element to the left being smaller, and every element to the right being larger).

If you can notice the wording of that is fairly similar to quicksort, you're correct. \textbf{Quickselect} is a selection algorithm that uses similar concepts like partitioning that quicksort uses, in order to find the $k$th smallest element by only partially sorting the array.

\subsubsection{Unimodal Searches}
\index{Unimodal Searches}

Less commonly, you may want to search the outputs of a function rather than an array. There are some search algorithms designed to search for maximums or minimums in a unimodal function (a function that has only one unique maximum or minimum).

\textbf{Ternary Search} \index{Ternary Search} is...

\textbf{Golden Section Search} \index{Golden Section Search} is...